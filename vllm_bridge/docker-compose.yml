x-env-files: &env_files
  - ./env/common.env
  - ../.env

x-offline-env: &offline_env
  HF_HUB_OFFLINE: "${HF_HUB_OFFLINE:-0}"
  TRANSFORMERS_OFFLINE: "${TRANSFORMERS_OFFLINE:-0}"
  
services:
  vllm_baseline:
    profiles: ["baseline"]
    image: vllm/vllm-openai:v0.9.0.1
    container_name: vllm_baseline
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    env_file: *env_files
    environment:
      <<: *offline_env
    volumes:
      - ${HF_CACHE_FOLDER}:/hf-cache
    ports:
      - "${VLLM_BASELINE_PORT:-8001}:8001"
    command:
      - "--model"
      - "${MODEL_NAME:-Qwen/Qwen3-0.6B}"
      - "--served-model-name"
      - "${MODEL_NAME:-Qwen/Qwen3-0.6B}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8001"
      # --- T4-safe knobs ---
      - "--dtype"
      - "float16"
      - "--gpu-memory-utilization"
      - "0.70"
      - "--max-num-seqs"
      - "16"
      - "--max-model-len"
      - "4096"
    restart: unless-stopped

  vllm_faststart:
    profiles: ["faststart"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: vllm_faststart
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]

    # ✅ IMPORTANT: share network namespace with sllm_store
    network_mode: "service:sllm_store"
    depends_on:
      sllm_store:
        condition: service_healthy

    env_file: *env_files
    environment:
      <<: *offline_env
    volumes:
      - ${MODEL_FOLDER}:/models
      - ${HF_CACHE_FOLDER}:/hf-cache

    # ❌ REMOVE THIS (ports must be on sllm_store when sharing network namespace)
    # ports:
    #   - "8000:8000"

    entrypoint: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    command:
      - "--model"
      - "/models/vllm/${MODEL_NAME:-Qwen/Qwen3-0.6B}"
      - "--served-model-name"
      - "${MODEL_NAME:-Qwen/Qwen3-0.6B}"
      - "--load-format"
      - "serverless_llm"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--dtype"
      - "float16"
      - "--gpu-memory-utilization"
      - "0.70"
      - "--max-num-seqs"
      - "16"
      - "--max-model-len"
      - "4096"
    restart: unless-stopped
      
  sllm_store:
    profiles: ["faststart"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: sllm_store
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    volumes:
      - ${MODEL_FOLDER}:/models
    env_file: *env_files
    environment:
      <<: *offline_env
    entrypoint: ["sllm-store"]
    command:
      - start
      - --storage-path
      - /models/vllm
      - --mem-pool-size
      - ${MEM_POOL_SIZE:-16GB}
      - --num-thread
      - ${SLLM_NUM_THREAD:-16}
      - --chunk-size
      - ${SLLM_CHUNK_SIZE:-128MB}

    # ✅ vLLM is listening on 8000 in this shared network namespace, so publish it from here.
    ports:
      - "${VLLM_FASTSTART_PORT:-8082}:8000"
      # Optional: you can expose store gRPC too, but not required for vLLM
      # - "8073:8073"

    # ✅ make depends_on actually mean “ready”
    healthcheck:
      test: ["CMD", "python3", "-c", "import socket; socket.create_connection(('127.0.0.1',8073),timeout=1).close()"]
      interval: 2s
      timeout: 1s
      retries: 180
      start_period: 60s

    restart: unless-stopped

  convert_model:
    profiles: ["tools"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: convert_model
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    env_file: *env_files
    environment:
      <<: *offline_env
    volumes:
      - ${MODEL_FOLDER}:/models
      - ${HF_CACHE_FOLDER}:/hf-cache
    entrypoint: ["python3", "/opt/sllm_tools/save_vllm_model.py"]
    command:
      - "--model-name"
      - "${MODEL_NAME:-Qwen/Qwen3-0.6B}"
      - "--tensor-parallel-size"
      - "1"
      - "--storage-path"
      - "/models/vllm"
    restart: "no"
