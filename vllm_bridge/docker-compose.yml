services:
  vllm_baseline:
    profiles: ["baseline"]
    image: vllm/vllm-openai:v0.9.0.1
    container_name: vllm_baseline
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    environment:
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${HF_CACHE_FOLDER}:/hf-cache
    ports:
      - "8001:8001"
    command:
      - "--model"
      - "Qwen/Qwen3-0.6B"
      - "--served-model-name"
      - "Qwen/Qwen3-0.6B"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8001"
      # --- T4-safe knobs ---
      - "--dtype"
      - "float16"
      - "--gpu-memory-utilization"
      - "0.70"
      - "--max-num-seqs"
      - "16"
      - "--max-model-len"
      - "4096"
    restart: unless-stopped

  vllm_faststart:
    profiles: ["faststart"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: vllm_faststart
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]

    # ✅ IMPORTANT: share network namespace with sllm_store
    network_mode: "service:sllm_store"
    depends_on:
      sllm_store:
        condition: service_healthy

    environment:
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${MODEL_FOLDER}:/models
      - ${HF_CACHE_FOLDER}:/hf-cache

    # ❌ REMOVE THIS (ports must be on sllm_store when sharing network namespace)
    # ports:
    #   - "8000:8000"

    entrypoint: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    command:
      - "--model"
      - "/models/vllm/Qwen/Qwen3-0.6B"
      - "--served-model-name"
      - "Qwen/Qwen3-0.6B"
      - "--load-format"
      - "serverless_llm"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--dtype"
      - "float16"
      - "--gpu-memory-utilization"
      - "0.70"
      - "--max-num-seqs"
      - "16"
      - "--max-model-len"
      - "4096"
    restart: unless-stopped
      
  sllm_store:
    profiles: ["faststart"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: sllm_store
    shm_size: "10gb"
    ipc: host
    volumes:
      - ${MODEL_FOLDER}:/models
    environment:
      - STORAGE_PATH=/models/vllm
      - MEM_POOL_SIZE=4GB

    entrypoint: ["sllm-store"]
    command: ["start", "--storage-path", "/models/vllm", "--mem-pool-size", "4GB"]

    # ✅ vLLM is listening on 8000 in this shared network namespace, so publish it from here.
    ports:
      - "8082:8000"
      # Optional: you can expose store gRPC too, but not required for vLLM
      # - "8073:8073"

    # ✅ make depends_on actually mean “ready”
    healthcheck:
      test: ["CMD", "python3", "-c", "import socket; socket.create_connection(('127.0.0.1',8073),timeout=1).close()"]
      interval: 2s
      timeout: 1s
      retries: 180
      start_period: 60s

    restart: unless-stopped

  convert_qwen3_0_6b:
    profiles: ["tools"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: convert_qwen3_0_6b
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    environment:
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${MODEL_FOLDER}:/models
      - ${HF_CACHE_FOLDER}:/hf-cache
    entrypoint: ["python3", "/opt/sllm_tools/save_vllm_model.py"]
    command:
      - "--model-name"
      - "Qwen/Qwen3-0.6B"
      - "--tensor-parallel-size"
      - "1"
      - "--storage-path"
      - "/models/vllm"
    restart: "no"
