services:
  vllm_baseline:
    profiles: ["baseline"]
    image: vllm/vllm-openai:v0.9.0.1
    container_name: vllm_baseline
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    environment:
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${HF_CACHE_FOLDER}:/hf-cache
    ports:
      - "8001:8001"
    command:
      - "--model"
      - "Qwen/Qwen3-0.6B"
      - "--served-model-name"
      - "Qwen/Qwen3-0.6B"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8001"
      # --- T4-safe knobs ---
      - "--dtype"
      - "float16"
      - "--gpu-memory-utilization"
      - "0.70"
      - "--max-num-seqs"
      - "16"
      - "--max-model-len"
      - "4096"
    restart: unless-stopped

  vllm_faststart:
    profiles: ["faststart"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: vllm_faststart
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    environment:
      - STORAGE_PATH=/models/vllm
      - MEM_POOL_SIZE=4GB
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${MODEL_FOLDER}:/models
      - ${HF_CACHE_FOLDER}:/hf-cache
    ports:
      - "8000:8000"
    # IMPORTANT: override image ENTRYPOINT (otherwise it starts store again)
    entrypoint: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    command:
      - "--model"
      - "/models/vllm/Qwen/Qwen3-0.6B"
      - "--served-model-name"
      - "Qwen/Qwen3-0.6B"
      - "--load-format"
      - "serverless_llm"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      # --- T4-safe knobs ---
      - "--dtype"
      - "float16"
      - "--gpu-memory-utilization"
      - "0.70"
      - "--max-num-seqs"
      - "16"
      - "--max-model-len"
      - "4096"
    restart: unless-stopped

  convert_qwen3_0_6b:
    profiles: ["tools"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: convert_qwen3_0_6b
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    environment:
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${MODEL_FOLDER}:/models
      - ${HF_CACHE_FOLDER}:/hf-cache
    entrypoint: ["python3", "/opt/sllm_tools/save_vllm_model.py"]
    command:
      - "--model-name"
      - "Qwen/Qwen3-0.6B"
      - "--tensor-parallel-size"
      - "1"
      - "--storage-path"
      - "/models/vllm"
    restart: "no"

  sllm_store:
    profiles: ["faststart"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: sllm_store
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    volumes:
      - ${MODEL_FOLDER}:/models
    environment:
      - STORAGE_PATH=/models/vllm
      - MEM_POOL_SIZE=4GB
    # IMPORTANT: override image ENTRYPOINT (otherwise it starts vLLM too)
    entrypoint: ["sllm-store"]
    command: ["start", "--storage-path", "/models/vllm", "--mem-pool-size", "4GB"]
    # IMPORTANT: expose vLLM port here because vllm_faststart will share this network namespace
    ports:
      - "8002:8000"
      - "8073:8073"
    restart: unless-stopped