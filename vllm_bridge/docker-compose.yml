services:
  vllm_baseline:
    profiles: ["baseline"]
    image: vllm/vllm-openai:v0.9.0.1
    container_name: vllm_baseline
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    environment:
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${HF_CACHE_FOLDER}:/hf-cache
    ports:
      - "8001:8001"
    command:
      - "--model"
      - "Qwen/Qwen3-0.6B"
      - "--served-model-name"
      - "Qwen/Qwen3-0.6B"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8001"
      # --- T4-safe knobs ---
      - "--dtype"
      - "float16"
      - "--gpu-memory-utilization"
      - "0.70"
      - "--max-num-seqs"
      - "16"
      - "--max-model-len"
      - "4096"
    restart: unless-stopped

  vllm_faststart:
    profiles: ["faststart"]
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: vllm_faststart
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    environment:
      - STORAGE_PATH=/models/vllm
      - MEM_POOL_SIZE=4GB
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${MODEL_FOLDER}:/models
      - ${HF_CACHE_FOLDER}:/hf-cache
    ports:
      - "8000:8000"
    command:
      - "--model"
      - "/models/vllm/Qwen/Qwen3-0.6B"
      - "--served-model-name"
      - "Qwen/Qwen3-0.6B"
      - "--load-format"
      - "serverless_llm"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      # --- T4-safe knobs ---
      - "--dtype"
      - "float16"
      - "--gpu-memory-utilization"
      - "0.70"
      - "--max-num-seqs"
      - "16"
      - "--max-model-len"
      - "4096"
    restart: unless-stopped

  convert_qwen3_0_6b:
    profiles: ["tools"]
    # Make sure the image exists even if you run convert first.
    build:
      context: ./infra
      dockerfile: Dockerfile.vllm_sllm
    image: sllm-vllm-faststart:v0.1
    container_name: convert_qwen3_0_6b
    shm_size: "10gb"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    environment:
      - HF_HOME=/hf-cache
      - HF_HUB_CACHE=/hf-cache/hub
      - TRANSFORMERS_CACHE=/hf-cache/transformers
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ${MODEL_FOLDER}:/models
      - ${HF_CACHE_FOLDER}:/hf-cache
    # IMPORTANT: exec-form entrypoint so args are not eaten by bash -c
    entrypoint: ["python3", "/opt/sllm_tools/save_vllm_model.py"]
    command:
      - "--model-name"
      - "Qwen/Qwen3-0.6B"
      - "--storage-path"
      - "/models/vllm"
    restart: "no"